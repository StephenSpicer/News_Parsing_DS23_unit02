{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gdsevBnrJebB"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi'] = 75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbQcM7b-JebF"
   },
   "source": [
    "# An Introduction to Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Sqe6gjPJebG"
   },
   "source": [
    "**Guest Lecturer:** Nicholas Cifuentes-Goodbody ([Twitter](https://twitter.com/ncgoodbody), [LinkedIn](https://www.linkedin.com/in/ncgoodbody/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjG6tUjSJebG"
   },
   "source": [
    "## What is NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rW7Vvz7JebG"
   },
   "source": [
    "Natural Language Processing (NLP) focuses on using machine learning to analyze human language. There are many differences between the **natural languages** that people use and **programming languages** that computers understand. That's why NLP tasks like voice recognition, language generation, and sentiment analysis are so challenging! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAaGbAjHJebG"
   },
   "source": [
    "## Our Task: Movie Review Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SdnlWWAJebH"
   },
   "source": [
    "![Rotten Tomatoes](https://github.com/ncgoodbody/nlp-lecture/blob/master/images/rotten-tomatoes-banner.jpg?raw=1)\n",
    "\n",
    "**What is sentiment analysis?** Sentiment analysis is when we use NLP techniques to study emotion in text or voice data. Our task today is to conduct sentiment analysis on movie reviews from [Rotten Tomatoes](https://www.rottentomatoes.com/). In this case, we'll build a model that predicts whether a reviewer feels positive or negative about a movie based on the text of their review.\n",
    "\n",
    "**Why do sentiment analysis on movie reviews?** Production and distribution companies often adjust their advertising strategies for a film after seeing how audiences react to it. Being able to gage public sentiment in social media venues like Twitter in real time would provide an enormous advantage to these companies. This is one way sentiment analysis is  used in the private sector.\n",
    "\n",
    "**What are the steps for doing sentiment analysis?** In today's lecture, we've broken down the process into three steps:\n",
    "\n",
    "- **1. Import and Explore:** In order to build our model, we'll have to evaluate the characteristics of our reviews.\n",
    "- **2. Preprocess:** Since we can't build a model with unstructured text data, we'll need to vectorize our dataset. \n",
    "- **Optional:** Remove *stop words* and add *n-grams*.\n",
    "- **3. Build and Train:** Once we have our vectors, we'll assemble a data pipeline, train our model, and evaluate its performance.\n",
    "\n",
    "**Did you say *vectorize*? What's that?** When you vectorize a text, you transform it into a numerical representation that is supposed to capture the meaning of the text. We'll be focusing on three strategies.\n",
    "\n",
    "| Strategy         | Strengths                                 | Weaknesses                                                               |\n",
    "| :----------------- | :---------------------------------------- | :----------------------------------------------------------------------- |\n",
    "| Count   | Good for small datasets.                  | Doesn't consider text length or term frequency. Slow for large datasets. |\n",
    "| TF-IDF   | Considers text length and term frequency. | Slow for large datasets.                                                 |\n",
    "| Hashing | Good for large datasets.                  | Low interpretablility. Not great on small datasets.                      |\n",
    "\n",
    "**Which Python tools will we use?** We'll use a popular machine learning library called [`scikit-learn`](http://scikit-learn.org/). Don't worry if you've never used it before. The goal of this lecture is for you to become comfortable with some of the *concepts* in NLP. This will speed up your learning when you start working with the tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1bWr5G2JebH"
   },
   "source": [
    "## Step 1: Import and Explore Our Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4vAxV73JebH"
   },
   "source": [
    "The dataset we'll use comes from [Kaggle](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data). (I've done some cleaning beforehand to speed things up.) Before we look at the data, there are a few vocabulary words that are important to know:\n",
    "\n",
    "- **Document:** Each text that counts as an observation in our dataset. A document can be of any length — as short as a tweet or as long as book. Here, our documents will be the individual movie reviews.\n",
    "- **Corpus:** The collection of documents that make up our dataset. Corpora can vary in size, but generally you'll need well over 10,000 documents to get good performance — and better to have 10 or 100 times that!\n",
    "\n",
    "Let's import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C7K_bG2AJebI"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/reviews.csv'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ea93a184e1ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/reviews.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1043\u001b[0m             )\n\u001b[0;32m   1044\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1045\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1860\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1861\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1863\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1864\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1355\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m         \"\"\"\n\u001b[1;32m-> 1357\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1358\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/reviews.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/reviews.csv')\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YWd8we0JebI"
   },
   "source": [
    "Looking at this `DataFrame`, we can see two things:\n",
    "\n",
    "1. Our corpus is pretty small, only 2,053 documents. This will be important to remember when we decide how to build our model.\n",
    "2. Our *features* (`'documents'`) and *targets* (`'sentiment'`) are in the same `DataFrame`. So the first thing we need to do is assign them to new variables, `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Z53CVs5JebI"
   },
   "outputs": [],
   "source": [
    "X = df['document']\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baHAK1-OJebI"
   },
   "source": [
    "Let's create a few plots so we can get a better idea of what our targets and features look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZlRT5qpvJebI"
   },
   "outputs": [],
   "source": [
    "# Explore the targets ('sentiment')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y.value_counts().plot(kind='bar')\n",
    "plt.xticks(rotation=0)\n",
    "plt.xlabel('sentiment')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('Category Frequencies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzn6JHIbJebI"
   },
   "source": [
    "Good news! The reviews are fairly evenly divided between positive and negative, so we don't need to worry about unbalanced classes. This will be important when we decide which metrics to use to evaluate our model.\n",
    "\n",
    "Next, let's look at the text of the reviews by creating some new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2NJQTG9JebJ"
   },
   "outputs": [],
   "source": [
    "# Explore the features ('document')\n",
    "\n",
    "# Create columns for character and word counts\n",
    "df['char_count'] = df['document'].apply(lambda x: len(x))\n",
    "df['word_count'] = df['document'].apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "# Plot our new columns\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "df['char_count'].plot(kind='hist', ax=ax1, title='character count')\n",
    "df['word_count'].plot(kind='hist', ax=ax2, title='word count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FhHr-cgJebJ"
   },
   "source": [
    "Here, we see that our reviews are short (about the length of a tweet). This is another important detail to consider when we build our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTFO-X8sJebJ"
   },
   "source": [
    "## Step 2: Preprocess Our Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0Rs7wkXJebJ"
   },
   "source": [
    "We have a problem: The predictors in `scikit-learn` need structured, numerical data. We have unstructured, text data. 😖 This means we need to **preprocess** our reviews first, transforming them into numerical representations that we can use for training. This task is one of the most important parts of NLP.\n",
    "\n",
    "Preprocessing generally has two steps:\n",
    "\n",
    "1. **Tokenization:** We divide our text into discrete units called **tokens**. Usually, this means splitting the text string on the `space` character and creating a list of individual words.\n",
    "2. **Vectorization:** We use our tokens to create a numerical vector of each document.\n",
    "\n",
    "`scikit-learn` will take care of the tokenization, so we're going to focus on vectorization. There are three options in `scikit-learn` to choose from. To better see the difference between them, let's create two \"mini-corpora\" from our dataset that we can play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IKVmWornJebJ"
   },
   "outputs": [],
   "source": [
    "# Mini-corpus 1: Short Reviews\n",
    "short_revs_idx = [312, 1307, 1143, 1699]\n",
    "short_revs = X[short_revs_idx].tolist()\n",
    "\n",
    "for n, review in enumerate(short_revs):\n",
    "    print(f'doc_{n}: {review}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8uu1eFJJebK"
   },
   "outputs": [],
   "source": [
    "# Mini-corpus 2: Bad Reviews\n",
    "bad_revs_ids = [1579, 2014, 1913, 1919]\n",
    "bad_revs = X[bad_revs_ids].to_list()\n",
    "\n",
    "for n, review in enumerate(bad_revs):\n",
    "    print(f'doc_{n}: {review}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEuGpJwqJebK"
   },
   "source": [
    "### `CountVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53acUXVCJebK"
   },
   "source": [
    "A [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) vectorizes text by counting how many times a token appears in each document. So, in this transformation, each row in our feature matrix corresponds to a document, and each column corresponds to a token in our mini-corpus.\n",
    "\n",
    "![CountVectorizer Animation](https://github.com/ncgoodbody/nlp-lecture/blob/master/images/cv.gif?raw=1)\n",
    "\n",
    "I'll do this transformation first to give you a model. Pay attention because you'll have to do similar transformations later in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lce4b80EJebK"
   },
   "outputs": [],
   "source": [
    "# Import vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate vectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Vectorize short_reviews\n",
    "short_revs_trans = cv.fit_transform(short_revs).toarray()\n",
    "\n",
    "# Create DataFrame\n",
    "df_cv = pd.DataFrame(short_revs_trans,\n",
    "                     columns=cv.get_feature_names())\n",
    "\n",
    "# Change index labels\n",
    "df_cv.index = [f'doc_{n}' for n in df_cv.index]\n",
    "\n",
    "# Show result\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFfY2p8dJebK"
   },
   "source": [
    "A `CountVectorizer` is good for small corpora (like ours) where the documents are roughly the same size (more on that in a second). One drawback, though, is that they don't capture word order. For instance, what if we switched the words `'bad'` and `'good'` in this review?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xzYUjATJebL"
   },
   "outputs": [],
   "source": [
    "print(bad_revs[2])\n",
    "print(bad_revs[2][0], bad_revs[2][-13:-9], bad_revs[2][6:-14], \n",
    "      bad_revs[2][2:5], bad_revs[2][-8:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4F6TEGkJebL"
   },
   "source": [
    "We have two reviews that have the opposite sentiment, but the vector representations would be exactly the same because they have the same tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKJKc1cXJebL"
   },
   "source": [
    "### `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "autB9k00JebL"
   },
   "source": [
    "TF-IDF stands \"term frequency, inverse document frequency\". The [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) has this name because improves on the `CountVectorizer` by taking two new characteristics into account: document length and prevalence of a term within the entire corpus.\n",
    "\n",
    "In order to see why these characteristics are important, let's look at what happens when we use a `CountVectorizer` to transform our `bad_reviews` mini-corpus.\n",
    "\n",
    "We'll do this transformation together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YKt3ee_JebL"
   },
   "outputs": [],
   "source": [
    "# Instantiate vectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Vectorize bad_reviews\n",
    "bad_revs_trans = cv.fit_transform(bad_revs).toarray()\n",
    "\n",
    "# Create DataFrame\n",
    "df_cv = pd.DataFrame(bad_revs_trans,\n",
    "                     columns=cv.get_feature_names())\n",
    "\n",
    "# Change index labels\n",
    "df_cv.index = [f'doc_{n}' for n in df_cv.index]\n",
    "\n",
    "# Show result\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XR0fhrg6JebM"
   },
   "source": [
    "Let's look at one token, the term `'bad'`. For context, we'll also add word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJW15bs4JebM"
   },
   "outputs": [],
   "source": [
    "# Add word count\n",
    "df_cv['word_count'] = [len(doc.split(' ')) for doc in bad_revs]\n",
    "\n",
    "# Filter columns\n",
    "df_cv[['word_count', 'bad']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5H8juG8JebM"
   },
   "source": [
    "The term `'bad'` occurs three times in `doc_3`. Does that mean that the sentiment is three times as negative? Not necessarily. `doc_3` is much longer than the other documents, so it's going to have a higher count for some terms. The `CountVecorizer` doesn't compensate for this.\n",
    "\n",
    "And what about this review from `short_revs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pfuox20KJebM"
   },
   "outputs": [],
   "source": [
    "print(short_revs[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-I07bir8JebM"
   },
   "source": [
    "In our corpus, `'garbage'` doesn't appear in many documents, and it's presence is a strong indicator of negative sentiment. In other words, the `'garbage'` token will be a helpful feature for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGhrucCmJebM"
   },
   "outputs": [],
   "source": [
    "df[df.document.str.contains('garbage')][['document', 'sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xn_Y0pvsJebM"
   },
   "source": [
    "But what if we had a different corpus, one full of documents about city sanitation? In that corpus, `'garbage'` would appear in nearly every document, and it's presence wouldn't be of much use to our model. Our `CountVectorizer` doesn't take this document frequency of terms into account either.\n",
    "\n",
    "The `TfidfVectorizer` tries to solve both of these problems by starting with a word count and then applying a weighting scheme that considers both how long a document is and how common each term is in the corpus. (You can check out how the math works [here](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting).)\n",
    "\n",
    "Let's see how TF-IDF changes the vectors for our `bad_reviews`. Try this one on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59pC4O0hJebN"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Instantiate vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Vectorize bad_revs\n",
    "bad_revs_trans = tfidf.fit_transform(bad_revs).toarray()\n",
    "\n",
    "# Create DataFrame for display below\n",
    "df_tfidf = pd.DataFrame(np.round(bad_revs_trans, 2),\n",
    "                        columns=tfidf.get_feature_names())\n",
    "\n",
    "# Change index labels\n",
    "df_tfidf.index = [f'doc_{n}' for n in df_tfidf.index]\n",
    "\n",
    "# Show result\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30M8kVWoJebN"
   },
   "source": [
    "Let's look again at the term `'bad'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1kYIPYrJebN"
   },
   "outputs": [],
   "source": [
    "# Add word count\n",
    "df_tfidf['word_count'] = [len(doc.split(' ')) for doc in bad_revs]\n",
    "\n",
    "# Filter columns\n",
    "df_tfidf[['word_count', 'bad']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0VLwasZJebN"
   },
   "source": [
    "We see that the `'bad'` count has been down-weighted to compensate for document length. Now it looks like `doc_0` is the more negative review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU8H3aBKJebN"
   },
   "source": [
    "---\n",
    "**Poll Question**\n",
    "\n",
    "- We saw earlier that the `CountVectorizer` doesn't take word order into account. Does the `TfidfVectorizer` fix this problem?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDn_PYN-JebN"
   },
   "source": [
    "### `HashingVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRGiwGyNJebN"
   },
   "source": [
    "To save time, we're not going to dive into the details of the [`HashingVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html). Here are the most important things you need to know.\n",
    "\n",
    "- They use [feature hashing](https://en.wikipedia.org/wiki/Hash_function) to create vectors.\n",
    "- Because of this, one advantage is that they're much faster than the `CountVectorizer` or `TfidfVectorizer` when working with big corpora.\n",
    "- One disadvantage is that there's no way to know which columns correspond to which tokens.\n",
    "- In some cases, they can hurt model performance when working with small corpora.\n",
    "\n",
    "Here's how you use scikit-learn's HashingVectorizer. Note that we don't need to call the `fit_transform` method, just `transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UA0ga34dJebO"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Instantiate vectorizer\n",
    "hv = HashingVectorizer(alternate_sign=False, norm=None)\n",
    "\n",
    "# Vectorize bad_revs\n",
    "bad_revs_trans = hv.transform(bad_revs).toarray()\n",
    "\n",
    "# Create DataFrame for display below\n",
    "df_hv = pd.DataFrame(bad_revs_trans)\n",
    "\n",
    "# Change index labels\n",
    "df_hv.index = [f'doc_{n}' for n in df_hv.index]\n",
    "\n",
    "# Show result\n",
    "df_hv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-w5waMwJebO"
   },
   "source": [
    "## Optional Steps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weDw9iV8JebP"
   },
   "source": [
    "Here are some other things you can do as part of preprocessing that can improve model performance. As we'll see, you can implement them by adding a few keyword arguments to your vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tv0l1-L-JebP"
   },
   "source": [
    "### Remove *Stop Words*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgMNLj5aJebP"
   },
   "source": [
    "Every language has words that are so common that they don't provide much predictive power to a model. In English, these are words like `'a'`, `'an'`, and `'the'`. Removing them during the tokenization process can improve our model. The three vectorizers we've seen have a `stop_words` argument that you can set to `'english'` and use a default list from `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ce1SENuWJebQ"
   },
   "outputs": [],
   "source": [
    "# Instantiate vectorizer\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Vectorize short_reviews\n",
    "short_revs_trans = cv.fit_transform(short_revs).toarray()\n",
    "\n",
    "# Create DataFrame\n",
    "df_cv = pd.DataFrame(short_revs_trans,\n",
    "                     columns=cv.get_feature_names())\n",
    "\n",
    "# Change index labels\n",
    "df_cv.index = [f'doc_{n}' for n in df_cv.index]\n",
    "\n",
    "# Show result\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWM-iKeBJebQ"
   },
   "source": [
    "### Add *n-grams*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rn_cnr4zJebQ"
   },
   "source": [
    "One strategy for solving the word order problem that we discussed above is to create new features for our data called **n-grams**. These are columns that correspond to *sequences of tokens*. All of the vectorizers we've seen have an `ngram_range` argument that allow us to add sequences. In this case, let's add sequences of two words — also called **bigrams** — to our data. This can improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VdMTyfyDJebQ"
   },
   "outputs": [],
   "source": [
    "# Instantiate vectorizer\n",
    "cv_ngram = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "# Vectorize our documents\n",
    "short_revs_trans = cv_ngram.fit_transform(short_revs)\n",
    "\n",
    "# Create DataFrame for display below\n",
    "df_ngram = pd.DataFrame(short_revs_trans.toarray(),\n",
    "                        columns=cv_ngram.get_feature_names())\n",
    "\n",
    "# Format index labels and column names\n",
    "df_ngram.index = [f'doc_{n}' for n in df_ngram.index]\n",
    "df_ngram.columns = [col.replace(' ', '_') for col in df_ngram.columns]\n",
    "\n",
    "# Show result\n",
    "df_ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7MbJc22JebQ"
   },
   "source": [
    "## Step 3: Create Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MtWI4H1JebQ"
   },
   "source": [
    "We've made it to the last step. Let's create our model! The first thing we'll do is decide as a group which vectorizer we should use.\n",
    "\n",
    "---\n",
    "\n",
    "**Poll Question**\n",
    "\n",
    "- We know from our exploratory data analysis that we have a small corpus with short documents. Given this, which of the three vectorizers do you think we should use?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HupDxtoJebQ"
   },
   "source": [
    "In addition to the vectorizer we've chosen, I've added some other components to the model. \n",
    "\n",
    "- [**`TruncatedSVD`**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.htm): This will take the output created by the vectorizer and reduce its size in a way that often improves performance. \n",
    "- [**`GradientBoostingClassifier`**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html): This predictor provides the flexibility of a decision tree but is more resistant to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bqw2Xw9JJebQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create pipeline\n",
    "model = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(lowercase=True, ngram_range=(1,1))),\n",
    "    ('dim_red', TruncatedSVD(n_components=50, random_state=42)),\n",
    "    ('predictor', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit model to training data\n",
    "model.fit(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzWhmvNLJebR"
   },
   "source": [
    "Once our model is fit, how can we evaluate its performance? One classification metric that most people know is *accuracy*. That's a good start (especially since our classes our pretty balances), but we should also look at [*precision* and *recall*](https://en.wikipedia.org/wiki/Precision_and_recall).\n",
    "\n",
    "**One important disclaimer:** If you really want to know how your model performs, you need to split your data into [*training* and *testing* sets](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets), and then compare metrics between the two. But we're more interested in the *concepts* in this lecture rather than model performance. The truth is our corpus is so small that our model would probably not generalize well. Remember, you need tens or hundreds of thousands of reviews to have a good model. So let's not worry about the split for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5fT4UZ5JebR"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zm47P9SJebR"
   },
   "source": [
    "Just for fun, let's try a few reviews that I've written. Feel free to write your own. See if you can fool the model. 😉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QbNhg3PXJebR"
   },
   "outputs": [],
   "source": [
    "test_revs = [\"This movie is terrible.\",\n",
    "             \"This movie is great.\",\n",
    "             \"I've seen worse films.\",\n",
    "             \"I think you should stay home instead.\"]\n",
    "\n",
    "model.predict(test_revs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RtHqQ_YJebR"
   },
   "source": [
    "## Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJI9PmpaJebR"
   },
   "source": [
    "- **Natural Language Processing** (NLP) uses machine learning to analyze human language.\n",
    "- **Sentiment analysis** is when you use NLP to study emotion in text or voice data.\n",
    "- In NLP, a dataset is called a **corpus**. It's comprised of **documents**.\n",
    "- In order to build an NLP model, you need to **preprocess** your corpus. \n",
    "- The two steps of preprocessing are **tokenization** and **vectorization**. \n",
    "- `scikit-learn` has three vectorization options:\n",
    "\n",
    "| Vectorizer          | Strengths                                                      | Weaknesses                                                                                                                 |\n",
    "| :------------------ | :------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `CountVectorizer`   | Good for small corpora where documents are of similar lengths. | Can be slow for large corpora. Doesn't consider document length or term frequency in corpus. Doesn't represent word order. |\n",
    "| `TfidfVectorizer`   | Good for small corpora. Takes into account document length and term frequency in corpus.                                       | Can be slow for large corpora. Doesn't represent word order.                                                               |\n",
    "| `HashingVectorizer` | Good for large corpora. Good for online learning.              | Can't explain features. Can hurt performance for small corpora. Doesn't represent word order.                              |\n",
    "\n",
    "- You might improve your model by removing **stop words**.\n",
    "- One way to represent word order is by adding **n-grams** to your features."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of lecture-answers.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/ncgoodbody/nlp-lecture/blob/master/lecture-answers.ipynb",
     "timestamp": 1611521204187
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}